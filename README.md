# DataStreamingModule

**ver1.0 :**


A model for data streaming that can be used in Big-Data scenarios. 

The mock data of this project was made to suit the template of ordinary e-shops, hence this project has adapt to it accordingly. The model should be able to deal with data streaming for online stores and e-shops as it was designed to deal with data came in the forms of JSON and from relational database.

Hasn't done any real-case test yet. 



## Mock-Data

It doesn't matter which type of data is intended to be streamed, as long as the modules changes accordingly.

This project is designed to deal with two types of data:

1. Business data that generated by actual processes of business, usually stored in databases before visualization.
2. log data, also known as behavior data, most likely in JSON.

The mock data jar is from :

[atguigu.com]: http://www.atguigu.com/	"you might need to know Chinese!"



log data are like this:

```json
["home","search","good_list","good_detail","login","good_detail","cart","trade","payment"]
```



Business data are often prestored in databases such as MySQL, the mock data that this project dealt with is the simplified model of ecommerce companies.

Table examples like : 

order_detail : order_id, sku_id, order_prices, img_url, sku_num, etc.

order_info : consignee, address_delivery, order_comment, tracking_no,etc. 

user_info : login_name, user_level, birthday, gender

etc.

## Data flow

**"WIP"**

## Framework implemented

MySQL 5.7

zookeeper

kafka       

redis 3.2

Hbase 2.0

ElasticSearch 6.6

Springboot

SparkStreaming

Canal 1.1.2

Mybatis

## Demands

To simulate the real business environment, four requirements were issued :

1. DAU : Daily Active User
2. GMV : Gross Merchandise Volume
3. Alert : a specific kind of stats that would arouse suspicion or indicating suspicious activities 
4. Query : quite straight forward, query. 

## Packages

### common  

to put all the constants.



### canal

to transfer the business data to kafka waiting to be processed.



### logger 

to receive the data send by clients, and store them to designated positions (in this project, kafka). 



### stream

to clean the original data and create the data flow to targeted software that is Hbase and ES in this case.

- When the endpoint is ES, remember to create index accordingly in ElasticSearch to be able to get the data

- it is rather common that sometimes the tables must be joined to be able to publish the required data

  

### publisher

to connect with visualization apps, where the real demands should be meet.



## Notes

1. **Has not been tested in real cases!**

2. This is  mostly used for personal tests.

3. Some of the annotations haven't been added yet.

   

   

 